{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27bJxGDBbJGo",
        "outputId": "3e8c6e47-087f-471a-a741-a88ef87c17ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357286 sha256=26e9c8edc0f2b21c2ade7fee9c9902e8ddd941ae00abd4aca4572148970ce017\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVD 1**"
      ],
      "metadata": {
        "id": "O6nyK3-fPe60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def prepare_data_csv(file_path, split_ratio=0.8):\n",
        "\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df.sort_values(by=['user_id', 'user_time'])\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "\n",
        "    for user_id, group in df.groupby('user_id'):\n",
        "        split_index = int(len(group) * split_ratio)\n",
        "        train_data_list.append(group.iloc[:split_index])\n",
        "        val_data_list.append(group.iloc[split_index:])\n",
        "\n",
        "    train_df = pd.concat(train_data_list).reset_index(drop=True)\n",
        "    val_df = pd.concat(val_data_list).reset_index(drop=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def prepare_data_model(train_df, val_df, rating_range=(1, 5)):\n",
        "\n",
        "    train_reader = Reader(rating_scale=rating_range)\n",
        "    val_reader = Reader(rating_scale=rating_range)\n",
        "\n",
        "    train_data = Dataset.load_from_df(train_df[[\"user_id\", \"movie_id\", \"rating\"]], train_reader)\n",
        "    valid_data = Dataset.load_from_df(val_df[[\"user_id\", \"movie_id\", \"rating\"]], val_reader)\n",
        "\n",
        "    return train_data, valid_data\n",
        "\n",
        "\n",
        "def train_model(train_data, model_name='SVD'):\n",
        "    model = SVD(n_factors=100, n_epochs=20, biased=True, lr_all=0.005, reg_all=0.02)\n",
        "    start_time = time.time()\n",
        "    training_set = train_data.build_full_trainset()\n",
        "    model.fit(training_set)\n",
        "    training_time = time.time() - start_time\n",
        "    training_time_ms = training_time * 1000\n",
        "\n",
        "\n",
        "\n",
        "    model_filename = f'{model_name}_movie_recommender.pkl'\n",
        "    with open(model_filename, 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)\n",
        "\n",
        "    print(f\"Model saved to {model_filename}\")\n",
        "    return model,training_time_ms\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "\n",
        "    dataset = [(rating[0], rating[1], rating[2]) for rating in data.raw_ratings]\n",
        "\n",
        "    predictions = model.test(dataset)\n",
        "\n",
        "    rmse = accuracy.rmse(predictions, verbose=True)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "def inference_cost_per_input(model, user_id, movie_id):\n",
        "    start_time = time.time()\n",
        "    model.predict(uid=user_id, iid=movie_id)\n",
        "    inference_time_seconds = time.time() - start_time\n",
        "    inference_time_ms = inference_time_seconds * 1000  # Convert to milliseconds\n",
        "    return inference_time_ms\n",
        "\n",
        "\n",
        "def predict(model, user_id, movie_list, user_movie_list, K=20):\n",
        "\n",
        "    recommendations = []\n",
        "    scores = []\n",
        "\n",
        "    for movie in movie_list:\n",
        "        if user_id in user_movie_list and movie in user_movie_list[user_id]:\n",
        "            continue\n",
        "        prediction = model.predict(uid=user_id, iid=movie)\n",
        "        scores.append((prediction.est, movie))\n",
        "\n",
        "    scores.sort(reverse=True)\n",
        "    recommendations = [movie for _, movie in scores[:K]]\n",
        "\n",
        "    return recommendations\n",
        "def get_model_size(model_filename):\n",
        "    # Get the size of the model in bytes\n",
        "    return os.path.getsize(model_filename)\n",
        "\n",
        "\n",
        "ratings_file = 'extracted_ratings.csv'\n",
        "\n",
        "train_df, val_df = prepare_data_csv(ratings_file)\n",
        "\n",
        "train_data, valid_data = prepare_data_model(train_df, val_df)\n",
        "\n",
        "model, training_time = train_model(train_data)\n",
        "\n",
        "rmse_score = evaluate(model, valid_data)\n",
        "print(f\"Validation RMSE value is {rmse_score}\")\n",
        "\n",
        "all_movies= train_df['movie_id'].unique().tolist()\n",
        "\n",
        "user_movies= train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
        "\n",
        "test_user_id = 93\n",
        "test_movie_id = train_df['movie_id'].iloc[0]\n",
        "recommendations = predict(model, test_user_id, all_movies, user_movies)\n",
        "print(f\"Top 20 recommendations for user {test_user_id}: {recommendations}\")\n",
        "\n",
        "inference_time_ms = inference_cost_per_input(model, test_user_id, test_movie_id)\n",
        "print(f\"Inference time per input: {inference_time_ms:.6f} milliseconds\")\n",
        "\n",
        "model_filename = 'SVD_movie_recommender.pkl'\n",
        "model_size_bytes = get_model_size(model_filename)\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "print(f\"Memory size of the model: {model_size_mb:.2f} MB\")\n",
        "\n",
        "print(f\"Training time: {training_time:.2f} milliseconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d43011a-a773-4fee-c587-1f69f022852a",
        "id": "bgFggjkTPXCj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to SVD_movie_recommender.pkl\n",
            "RMSE: 0.8287\n",
            "Validation RMSE value is 0.8286712240397239\n",
            "Top 20 recommendations for user 93: ['the+shawshank+redemption+1994', 'the+lives+of+others+2006', 'das+boot+1981', 'seven+samurai+1954', 'city+of+god+2002', 'the+godfather+1972', 'the+usual+suspects+1995', 'reservoir+dogs+1992', 'american+history+x+1998', 'the+prestige+2006', 'life+is+beautiful+1997', 'memento+2000', 'schindlers+list+1993', 'dr.+strangelove+or+how+i+learned+to+stop+worrying+and+love+the+bomb+1964', 'fight+club+1999', 'shallow+grave+1994', 'amlie+2001', 'the+african+queen+1951', 'the+wrong+trousers+1993', 'my+neighbor+totoro+1988']\n",
            "Inference time per input: 0.012159 milliseconds\n",
            "Memory size of the model: 15.21 MB\n",
            "Training time: 411.73 milliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SVD 2**"
      ],
      "metadata": {
        "id": "I-I-KlnjPRz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def prepare_data_csv(file_path, split_ratio=0.8):\n",
        "\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df.sort_values(by=['user_id', 'user_time'])\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "\n",
        "    for user_id, group in df.groupby('user_id'):\n",
        "        split_index = int(len(group) * split_ratio)\n",
        "        train_data_list.append(group.iloc[:split_index])\n",
        "        val_data_list.append(group.iloc[split_index:])\n",
        "\n",
        "    train_df = pd.concat(train_data_list).reset_index(drop=True)\n",
        "    val_df = pd.concat(val_data_list).reset_index(drop=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def prepare_data_model(train_df, val_df, rating_range=(1, 5)):\n",
        "\n",
        "    train_reader = Reader(rating_scale=rating_range)\n",
        "    val_reader = Reader(rating_scale=rating_range)\n",
        "\n",
        "    train_data = Dataset.load_from_df(train_df[[\"user_id\", \"movie_id\", \"rating\"]], train_reader)\n",
        "    valid_data = Dataset.load_from_df(val_df[[\"user_id\", \"movie_id\", \"rating\"]], val_reader)\n",
        "\n",
        "    return train_data, valid_data\n",
        "\n",
        "\n",
        "def train_model(train_data, model_name='SVD'):\n",
        "    model = SVD(n_factors=150, n_epochs=30, biased=True, lr_all=0.01, reg_all=0.05)\n",
        "    start_time = time.time()\n",
        "    training_set = train_data.build_full_trainset()\n",
        "    model.fit(training_set)\n",
        "    training_time = time.time() - start_time\n",
        "    training_time_ms = training_time * 1000\n",
        "\n",
        "\n",
        "\n",
        "    model_filename = f'{model_name}_movie_recommender.pkl'\n",
        "    with open(model_filename, 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)\n",
        "\n",
        "    print(f\"Model saved to {model_filename}\")\n",
        "    return model,training_time_ms\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "\n",
        "    dataset = [(rating[0], rating[1], rating[2]) for rating in valid_data.raw_ratings]\n",
        "\n",
        "    predictions = model.test(dataset)\n",
        "\n",
        "    rmse = accuracy.rmse(predictions, verbose=True)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "def inference_cost_per_input(model, user_id, movie_id):\n",
        "    start_time = time.time()\n",
        "    model.predict(uid=user_id, iid=movie_id)\n",
        "    inference_time_seconds = time.time() - start_time\n",
        "    inference_time_ms = inference_time_seconds * 1000  # Convert to milliseconds\n",
        "    return inference_time_ms\n",
        "\n",
        "\n",
        "def predict(model, user_id, movie_list, user_movie_list, K=20):\n",
        "\n",
        "    recommendations = []\n",
        "    scores = []\n",
        "\n",
        "    for movie in movie_list:\n",
        "        if user_id in user_movie_list and movie in user_movie_list[user_id]:\n",
        "            continue\n",
        "        prediction = model.predict(uid=user_id, iid=movie)\n",
        "        scores.append((prediction.est, movie))\n",
        "\n",
        "    scores.sort(reverse=True)\n",
        "    recommendations = [movie for _, movie in scores[:K]]\n",
        "\n",
        "    return recommendations\n",
        "def get_model_size(model_filename):\n",
        "    # Get the size of the model in bytes\n",
        "    return os.path.getsize(model_filename)\n",
        "\n",
        "\n",
        "ratings_file = 'extracted_ratings.csv'\n",
        "\n",
        "train_df, val_df = prepare_data_csv(ratings_file)\n",
        "\n",
        "train_data, valid_data = prepare_data_model(train_df, val_df)\n",
        "\n",
        "model, training_time = train_model(train_data)\n",
        "\n",
        "rmse_score = evaluate(model, valid_data)\n",
        "print(f\"Validation RMSE value is {rmse_score}\")\n",
        "\n",
        "all_movies= train_df['movie_id'].unique().tolist()\n",
        "\n",
        "user_movies= train_df.groupby('user_id')['movie_id'].apply(set).to_dict()\n",
        "\n",
        "test_user_id = 93\n",
        "test_movie_id = train_df['movie_id'].iloc[0]\n",
        "recommendations = predict(model, test_user_id, all_movies, user_movies)\n",
        "print(f\"Top 20 recommendations for user {test_user_id}: {recommendations}\")\n",
        "\n",
        "inference_time_ms = inference_cost_per_input(model, test_user_id, test_movie_id)\n",
        "print(f\"Inference time per input: {inference_time_ms:.6f} milliseconds\")\n",
        "\n",
        "model_filename = 'SVD_movie_recommender.pkl'\n",
        "model_size_bytes = get_model_size(model_filename)\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "print(f\"Memory size of the model: {model_size_mb:.2f} MB\")\n",
        "\n",
        "print(f\"Training time: {training_time:.2f} milliseconds\")"
      ],
      "metadata": {
        "id": "HWqpX3Njpbwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138e792f-7e9d-4041-a4e9-f0b7ef448e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to SVD_movie_recommender.pkl\n",
            "RMSE: 0.8302\n",
            "Validation RMSE value is 0.8301501547506631\n",
            "Top 20 recommendations for user 93: ['the+lives+of+others+2006', 'the+shawshank+redemption+1994', 'das+boot+1981', 'gran+torino+2008', 'dancer+in+the+dark+2000', 'seven+samurai+1954', 'shallow+grave+1994', 'henry+v+1989', 'city+of+god+2002', 'the+prestige+2006', 'the+godfather+1972', 'oldboy+2003', 'memento+2000', 'life+is+beautiful+1997', 'the+usual+suspects+1995', 'dr.+strangelove+or+how+i+learned+to+stop+worrying+and+love+the+bomb+1964', 'spellbound+2002', 'everest+1998', 'persuasion+1995', 'pride++prejudice+2005']\n",
            "Inference time per input: 0.037432 milliseconds\n",
            "Memory size of the model: 21.98 MB\n",
            "Training time: 1506.19 milliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01vfVtMhbcCv",
        "outputId": "8e49fc88-98d3-40a0-d9c8-b56292959702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vka0v8QRCoTA",
        "outputId": "208af563-b078-4925-8090-6210dbbeda19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *SURPRISE MODEL- Experimental*"
      ],
      "metadata": {
        "id": "MUzWv-z5sYxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_features(age, gender, occupation, le_gender, le_occupation, scaler):\n",
        "    age_scaled = scaler.transform([[age]])[0][0] if age is not None else 0\n",
        "    gender_encoded = le_gender.transform([gender])[0] if gender in le_gender.classes_ else -1\n",
        "    occupation_encoded = le_occupation.transform([occupation])[0] if occupation in le_occupation.classes_ else -1\n",
        "    return np.array([age_scaled, gender_encoded, occupation_encoded])\n",
        "\n",
        "\n",
        "def predict_rating(base_est, user_features, movie_features, mean_user_factors, user_bias, item_bias, n_factors):\n",
        "    padded_features = np.pad(np.concatenate([user_features, movie_features]),\n",
        "                             (0, max(0, n_factors - len(user_features) - len(movie_features))))\n",
        "    adjustment = np.dot(padded_features, mean_user_factors) + user_bias + item_bias\n",
        "    prediction = base_est + 0.01 * adjustment\n",
        "    return max(0, min(1, prediction))\n",
        "\n",
        "def preprocess_movie_features(movie_details, tfidf):\n",
        "    movie_details['genres'] = movie_details['genres'].fillna('')\n",
        "    genres_vector = tfidf.transform(movie_details['genres']).toarray()\n",
        "    return dict(zip(movie_details['movie_id'], genres_vector))\n",
        "\n",
        "def get_top_n_recommendations(model, user_id, age, gender, occupation, movie_features, le_gender, le_occupation, scaler, mean_user_factors, user_bias, item_bias, n=20):\n",
        "    user_features = get_user_features(age, gender, occupation, le_gender, le_occupation, scaler)\n",
        "    n_factors = model.pu.shape[1]\n",
        "\n",
        "    def process_movie(movie_id_vector):\n",
        "        movie_id, movie_vector = movie_id_vector\n",
        "        try:\n",
        "            base_est = model.predict(user_id, movie_id).est\n",
        "        except ValueError:\n",
        "            base_est = model.trainset.global_mean\n",
        "\n",
        "        rating = predict_rating(base_est, user_features, movie_vector, mean_user_factors, user_bias, item_bias, n_factors)\n",
        "        return movie_id, rating + np.random.normal(0, 0.001)\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        user_ratings = list(executor.map(process_movie, movie_features.items()))\n",
        "\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    return user_ratings[:n]\n",
        "\n",
        "def save_model_and_encoders(model, movie_to_index, le_gender, le_occupation, scaler, movie_features, tfidf, path='recommendation_model.pkl'):\n",
        "    mean_user_factors = np.mean(model.pu, axis=0)\n",
        "    user_bias = np.mean(model.bu)\n",
        "    item_bias = np.mean(model.bi)\n",
        "\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'svd_model': model,\n",
        "            'movie_to_index': movie_to_index,\n",
        "            'le_gender': le_gender,\n",
        "            'le_occupation': le_occupation,\n",
        "            'scaler': scaler,\n",
        "            'tfidf': tfidf,\n",
        "            'movie_features': movie_features,\n",
        "            'mean_user_factors': mean_user_factors,\n",
        "            'user_bias': user_bias,\n",
        "            'item_bias': item_bias\n",
        "        }, f)\n",
        "    print(\"Model and data saved successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def server_predict(user_id, age, gender, occupation):\n",
        "    start_time = time.time()\n",
        "\n",
        "    with open('recommendation_model.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    model = data['svd_model']\n",
        "    le_gender = data['le_gender']\n",
        "    le_occupation = data['le_occupation']\n",
        "    scaler = data['scaler']\n",
        "    movie_features = data['movie_features']\n",
        "    mean_user_factors = data['mean_user_factors']\n",
        "    user_bias = data['user_bias']\n",
        "    item_bias = data['item_bias']\n",
        "\n",
        "    recommendations = get_top_n_recommendations(\n",
        "        model, user_id, age, gender, occupation, movie_features,\n",
        "        le_gender, le_occupation, scaler, mean_user_factors, user_bias, item_bias\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = (end_time - start_time) * 1000\n",
        "    print(f\"Total prediction time: {total_time:.2f} ms\")\n",
        "\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "9xlZPkErYhxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "import concurrent.futures\n",
        "import time\n",
        "\n",
        "ratings = pd.read_csv('extracted_ratings.csv')\n",
        "watches = pd.read_csv('extracted_watches.csv')\n",
        "movie_details = pd.read_csv('movie_details.csv')\n",
        "user_details = pd.read_csv('user_details.csv')\n",
        "\n",
        "ratings['interaction'] = 1\n",
        "watches['interaction'] = 1\n",
        "interactions = pd.concat([\n",
        "        ratings[['user_id', 'movie_id', 'interaction']],\n",
        "        watches[['user_id', 'movie_id', 'interaction']]\n",
        "    ]).drop_duplicates()\n",
        "\n",
        "le_gender = LabelEncoder()\n",
        "le_occupation = LabelEncoder()\n",
        "user_details['gender_encoded'] = le_gender.fit_transform(user_details['gender'])\n",
        "user_details['occupation_encoded'] = le_occupation.fit_transform(user_details['occupation'])\n",
        "scaler = StandardScaler()\n",
        "user_details[['age_scaled']] = scaler.fit_transform(user_details[['age']])\n",
        "\n",
        "movie_details['genres'] = movie_details['genres'].fillna('')\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "movie_details['genres_vector'] = tfidf.fit_transform(movie_details['genres']).toarray().tolist()\n",
        "\n",
        "interactions = interactions.merge(user_details, on='user_id', how='left')\n",
        "interactions = interactions.merge(movie_details[['movie_id', 'genres_vector']], on='movie_id', how='left')\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "data = Dataset.load_from_df(interactions[['user_id', 'movie_id', 'interaction']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training the model...\")\n",
        "model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
        "model.fit(trainset)\n",
        "\n",
        "print(\"Preprocessing movie features...\")\n",
        "movie_to_index = {movie: idx for idx, movie in enumerate(movie_details['movie_id'].unique())}\n",
        "movie_features = preprocess_movie_features(movie_details, tfidf)\n",
        "\n",
        "print(\"Saving the model and preprocessed data...\")\n",
        "save_model_and_encoders(model, movie_to_index, le_gender, le_occupation, scaler, movie_features, tfidf)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nServer-side prediction example:\")\n",
        "# server_recommendations = server_predict('33039', 30, 'M', 'homemaker')\n",
        "server_recommendations = server_predict('93', 55, 'F', 'scientist')\n",
        "print(\"\\nTop 20 recommendations:\")\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05294dfb-16ad-4e08-f492-ca8ed3b03a5f",
        "id": "LTHVCj2PYDBF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Preprocessing movie features...\n",
            "Saving the model and preprocessed data...\n",
            "Model and data saved successfully.\n",
            "\n",
            "Server-side prediction example:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total prediction time: 844.14 ms\n",
            "\n",
            "Top 20 recommendations:\n",
            "Movie ID: neighbors+1920, Predicted Score: 1.0037\n",
            "Movie ID: the+house+that+dripped+blood+1971, Predicted Score: 1.0034\n",
            "Movie ID: trouble+in+paradise+1932, Predicted Score: 1.0033\n",
            "Movie ID: our+lady+of+the+assassins+2000, Predicted Score: 1.0032\n",
            "Movie ID: fatal+instinct+1993, Predicted Score: 1.0031\n",
            "Movie ID: no+greater+love+2009, Predicted Score: 1.0030\n",
            "Movie ID: robin+hood+men+in+tights+1993, Predicted Score: 1.0030\n",
            "Movie ID: future+weather+2012, Predicted Score: 1.0029\n",
            "Movie ID: twilights+last+gleaming+1977, Predicted Score: 1.0029\n",
            "Movie ID: plainlands+1988, Predicted Score: 1.0028\n",
            "Movie ID: autumn+spring+2001, Predicted Score: 1.0028\n",
            "Movie ID: the+wog+boy+2000, Predicted Score: 1.0027\n",
            "Movie ID: resurrect+dead+the+mystery+of+the+toynbee+tiles+2011, Predicted Score: 1.0027\n",
            "Movie ID: backbeat+1994, Predicted Score: 1.0027\n",
            "Movie ID: return+of+the+jedi+1983, Predicted Score: 1.0027\n",
            "Movie ID: dream+wife+1953, Predicted Score: 1.0027\n",
            "Movie ID: ride+the+divide+2010, Predicted Score: 1.0026\n",
            "Movie ID: halloween+1978, Predicted Score: 1.0026\n",
            "Movie ID: the+drowning+pool+1975, Predicted Score: 1.0026\n",
            "Movie ID: the+best+offer+2013, Predicted Score: 1.0026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nServer-side prediction example:\")\n",
        "server_recommendations = server_predict('33039', 30, 'M', 'homemaker')\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGWyf8iMKrqr",
        "outputId": "094865d9-2ae1-47ea-8384-d5f1de0a9846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Server-side prediction example:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total prediction time: 866.69 ms\n",
            "Movie ID: bedtime+stories+2008, Predicted Score: 1.0036\n",
            "Movie ID: the+inheritance+2003, Predicted Score: 1.0035\n",
            "Movie ID: my+way+2011, Predicted Score: 1.0032\n",
            "Movie ID: capturing+the+friedmans+2003, Predicted Score: 1.0031\n",
            "Movie ID: nausica+of+the+valley+of+the+wind+1984, Predicted Score: 1.0031\n",
            "Movie ID: nightmare+man+2006, Predicted Score: 1.0030\n",
            "Movie ID: mouth+to+mouth+2005, Predicted Score: 1.0029\n",
            "Movie ID: queen+days+of+our+lives+2011, Predicted Score: 1.0029\n",
            "Movie ID: gumshoe+1971, Predicted Score: 1.0029\n",
            "Movie ID: in+the+hands+of+the+gods+2007, Predicted Score: 1.0028\n",
            "Movie ID: lars+and+the+real+girl+2007, Predicted Score: 1.0028\n",
            "Movie ID: mans+job+2007, Predicted Score: 1.0028\n",
            "Movie ID: the+scarlet+clue+1945, Predicted Score: 1.0028\n",
            "Movie ID: lovely+molly+2012, Predicted Score: 1.0027\n",
            "Movie ID: the+girl+who+kicked+the+hornets+nest+2009, Predicted Score: 1.0027\n",
            "Movie ID: bunty+aur+babli+2005, Predicted Score: 1.0027\n",
            "Movie ID: high+tension+2003, Predicted Score: 1.0027\n",
            "Movie ID: my+neighbor+totoro+1988, Predicted Score: 1.0026\n",
            "Movie ID: the+myth+2005, Predicted Score: 1.0026\n",
            "Movie ID: the+olsen+gang+1968, Predicted Score: 1.0026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "server_recommendations = server_predict('6666', 27, 'M', 'scientist')\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY27RLjU2NiU",
        "outputId": "c189b83b-0920-46a9-dcc4-7c78f1e6ac40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model load time: 41.47 ms\n",
            "Inference time: 424.41 ms\n",
            "Total prediction time: 465.90 ms\n",
            "Movie ID: the+journey+1992, Predicted Score: 1.0035\n",
            "Movie ID: pandoras+box+1929, Predicted Score: 1.0033\n",
            "Movie ID: the+americanization+of+emily+1964, Predicted Score: 1.0032\n",
            "Movie ID: the+caller+2011, Predicted Score: 1.0031\n",
            "Movie ID: arabian+nights+1974, Predicted Score: 1.0030\n",
            "Movie ID: masti+2004, Predicted Score: 1.0030\n",
            "Movie ID: virtuosity+1995, Predicted Score: 1.0029\n",
            "Movie ID: midaq+alley+1995, Predicted Score: 1.0029\n",
            "Movie ID: bad+boy+bubby+1993, Predicted Score: 1.0029\n",
            "Movie ID: shine+a+light+2008, Predicted Score: 1.0028\n",
            "Movie ID: pitch+black+2000, Predicted Score: 1.0028\n",
            "Movie ID: tinker+tailor+soldier+spy+1979, Predicted Score: 1.0028\n",
            "Movie ID: sharpes+sword+1995, Predicted Score: 1.0028\n",
            "Movie ID: night+of+the+ghouls+1959, Predicted Score: 1.0028\n",
            "Movie ID: time+regained+1999, Predicted Score: 1.0027\n",
            "Movie ID: capitalism+a+love+story+2009, Predicted Score: 1.0027\n",
            "Movie ID: the+botany+of+desire+2009, Predicted Score: 1.0027\n",
            "Movie ID: pauline+dtective+2012, Predicted Score: 1.0027\n",
            "Movie ID: sunshine+on+leith+2013, Predicted Score: 1.0027\n",
            "Movie ID: the+two+escobars+2010, Predicted Score: 1.0027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "server_recommendations = server_predict('466', 27, 'F', 'K-12 student')\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840390a2-375e-410a-a97c-96ece21a1d81",
        "id": "iY2QWJPI3v2j"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model load time: 154.16 ms\n",
            "Inference time: 192.22 ms\n",
            "Total prediction time: 346.38 ms\n",
            "Movie ID: the+night+they+raided+minskys+1968, Predicted Score: 1.0037\n",
            "Movie ID: the+joker+is+wild+1957, Predicted Score: 1.0035\n",
            "Movie ID: premium+rush+2012, Predicted Score: 1.0033\n",
            "Movie ID: the+prestige+2006, Predicted Score: 1.0030\n",
            "Movie ID: the+sexual+life+of+the+belgians+1994, Predicted Score: 1.0029\n",
            "Movie ID: charly+1968, Predicted Score: 1.0029\n",
            "Movie ID: horse+feathers+1932, Predicted Score: 1.0029\n",
            "Movie ID: waxworks+1924, Predicted Score: 1.0029\n",
            "Movie ID: in++out+1997, Predicted Score: 1.0028\n",
            "Movie ID: inside+llewyn+davis+2013, Predicted Score: 1.0028\n",
            "Movie ID: the+intouchables+2011, Predicted Score: 1.0028\n",
            "Movie ID: blink+1994, Predicted Score: 1.0028\n",
            "Movie ID: the+kiss+of+the+vampire+1963, Predicted Score: 1.0028\n",
            "Movie ID: his+girl+friday+1940, Predicted Score: 1.0028\n",
            "Movie ID: pierrot+le+fou+1965, Predicted Score: 1.0028\n",
            "Movie ID: nobody+loves+me+1994, Predicted Score: 1.0028\n",
            "Movie ID: the+twilight+people+1972, Predicted Score: 1.0027\n",
            "Movie ID: demons+1985, Predicted Score: 1.0027\n",
            "Movie ID: dark+water+2002, Predicted Score: 1.0027\n",
            "Movie ID: killing+kasztner+2008, Predicted Score: 1.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UJ5UV0J04FIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SECOND MODEL-METRICS-Experimental"
      ],
      "metadata": {
        "id": "BGNW_9qi3rk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_features, movie_indices, labels in test_loader:\n",
        "            user_features, movie_indices, labels = user_features.to(device), movie_indices.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(user_features, movie_indices)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    binary_preds = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    # RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
        "\n",
        "    precision = precision_score(all_labels, binary_preds)\n",
        "    recall = recall_score(all_labels, binary_preds)\n",
        "    f1 = f1_score(all_labels, binary_preds)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    return avg_loss, rmse, precision, recall, f1\n",
        "\n",
        "test_loss, test_rmse, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvmwOv8f3c2u",
        "outputId": "35fb3ab7-09a1-4f39-e429-6ca5aafbebe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0877\n",
            "Test RMSE: 0.1313\n",
            "Test Precision: 1.0000\n",
            "Test Recall: 0.9796\n",
            "Test F1-Score: 0.9897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SECOND MODEL SVD with parameter changed-Experimental**"
      ],
      "metadata": {
        "id": "z1U8Ekqg3qn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "class SimpleCollaborativeFiltering(nn.Module):\n",
        "    def __init__(self, num_movies, num_user_features, embedding_dim=400):\n",
        "        super(SimpleCollaborativeFiltering, self).__init__()\n",
        "        self.user_features = nn.Linear(num_user_features, embedding_dim)\n",
        "        self.movie_embeddings = nn.Embedding(num_movies, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, user_features, movie_indices):\n",
        "        user_embed = self.relu(self.user_features(user_features))\n",
        "        movie_embed = self.movie_embeddings(movie_indices)\n",
        "        x = torch.cat([user_embed, movie_embed], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x).squeeze()\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, user_features, movie_indices, interaction_labels):\n",
        "        self.user_features = torch.tensor(user_features, dtype=torch.float)\n",
        "        self.movie_indices = torch.tensor(movie_indices, dtype=torch.long)\n",
        "        self.labels = torch.tensor(interaction_labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_features[idx], self.movie_indices[idx], self.labels[idx]\n",
        "\n",
        "def preprocess_data():\n",
        "    ratings = pd.read_csv('extracted_ratings.csv')\n",
        "    watches = pd.read_csv('extracted_watches.csv')\n",
        "    movie_details = pd.read_csv('movie_details.csv')\n",
        "    user_details = pd.read_csv('user_details.csv')\n",
        "\n",
        "    ratings['interaction'] = 1\n",
        "    watches['interaction'] = 1\n",
        "    interactions = pd.concat([ratings[['user_id', 'movie_id', 'interaction']],\n",
        "                              watches[['user_id', 'movie_id', 'interaction']]]).drop_duplicates()\n",
        "\n",
        "    interactions = interactions.merge(user_details, on='user_id', how='left')\n",
        "\n",
        "    interactions = interactions.merge(movie_details[['movie_id', 'title']], on='movie_id', how='left')\n",
        "\n",
        "    interactions.dropna(subset=['age', 'gender', 'occupation', 'movie_id'], inplace=True)\n",
        "\n",
        "    le_gender = LabelEncoder()\n",
        "    le_occupation = LabelEncoder()\n",
        "\n",
        "    interactions['gender'] = le_gender.fit_transform(interactions['gender'])\n",
        "    interactions['occupation'] = le_occupation.fit_transform(interactions['occupation'])\n",
        "    interactions['age'] = (interactions['age'] - interactions['age'].mean()) / interactions['age'].std()\n",
        "    movie_to_index = {movie: idx for idx, movie in enumerate(interactions['movie_id'].unique())}\n",
        "    interactions['movie_idx'] = interactions['movie_id'].map(movie_to_index)\n",
        "\n",
        "    return interactions, movie_to_index, le_gender, le_occupation\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info().rss  # in bytes\n",
        "    memory_in_mb = memory_info / (1024 ** 2)  # Convert to MB\n",
        "    return memory_in_mb\n",
        "\n",
        "interactions, movie_to_index, le_gender, le_occupation = preprocess_data()\n",
        "\n",
        "train_data, test_data = train_test_split(interactions, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = InteractionDataset(train_data[['age', 'gender', 'occupation']].values,\n",
        "                                   train_data['movie_idx'].values, train_data['interaction'].values)\n",
        "test_dataset = InteractionDataset(test_data[['age', 'gender', 'occupation']].values,\n",
        "                                  test_data['movie_idx'].values, test_data['interaction'].values)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "num_movies = len(movie_to_index)\n",
        "num_user_features = 3  # age, gender, occupation\n",
        "model = SimpleCollaborativeFiltering(num_movies, num_user_features)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# start the timer before the first training epoch\n",
        "start_time = time.time()\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for user_features, movie_indices, labels in train_loader:\n",
        "        user_features, movie_indices, labels = user_features.to(device), movie_indices.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(user_features, movie_indices)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# end the timer after all training\n",
        "end_time = time.time()\n",
        "evaluation_duration_ms = (end_time - start_time) * 1000\n",
        "print(f\"Total evaluation time: {evaluation_duration_ms:.2f} milliseconds\")\n",
        "print(f\"Memory usage after training: {get_memory_usage():.2f} MB\")\n",
        "print(\"------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "def save_model_and_encoders(model, movie_to_index, le_gender, le_occupation, scaler, path='collaborative_filtering.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'movie_to_index': movie_to_index,\n",
        "        'le_gender': le_gender,\n",
        "        'le_occupation': le_occupation,\n",
        "        'scaler': scaler\n",
        "    }, path)\n",
        "\n",
        "interactions, movie_to_index, le_gender, le_occupation = preprocess_data()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "interactions[['age']] = scaler.fit_transform(interactions[['age']])\n",
        "\n",
        "save_model_and_encoders(model, movie_to_index, le_gender, le_occupation, scaler)\n",
        "\n",
        "#USE THIS PART IN CALLING THE MODEL FOR PREDICTIONS\n",
        "\n",
        "def load_model(path='collaborative_filtering.pth'):\n",
        "    checkpoint = torch.load(path)\n",
        "    num_movies = len(checkpoint['movie_to_index'])\n",
        "    model = SimpleCollaborativeFiltering(num_movies, 3)  # 3 for user features: age, gender, occupation\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model, checkpoint\n",
        "\n",
        "def preprocess_user(age, occupation, gender, checkpoint):\n",
        "    le_gender = checkpoint['le_gender']\n",
        "    le_occupation = checkpoint['le_occupation']\n",
        "    scaler = checkpoint['scaler']\n",
        "\n",
        "    gender_encoded = le_gender.transform([gender])[0]\n",
        "    occupation_encoded = le_occupation.transform([occupation])[0]\n",
        "    age_normalized = scaler.transform([[age]])[0][0]\n",
        "\n",
        "    user_features = torch.tensor([age_normalized, gender_encoded, occupation_encoded], dtype=torch.float)\n",
        "    return user_features\n",
        "\n",
        "def get_top_n_recommendations(model, user_features, movie_to_index, n=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        user_features = user_features.unsqueeze(0).repeat(len(movie_to_index), 1)\n",
        "        movie_indices = torch.tensor(list(movie_to_index.values()))\n",
        "        predictions = model(user_features, movie_indices)\n",
        "\n",
        "    top_n_indices = predictions.argsort(descending=True)[:n]\n",
        "\n",
        "    index_to_movie = {idx: movie for movie, idx in movie_to_index.items()}\n",
        "    top_n_movies = [index_to_movie[idx.item()] for idx in top_n_indices]\n",
        "    end_time = time.time()\n",
        "    latency = (end_time - start_time) * 1000\n",
        "    print(f\"Inference latency: {latency:.2f} milliseconds\")\n",
        "\n",
        "    return top_n_movies, predictions[top_n_indices].numpy()\n",
        "\n",
        "model, checkpoint = load_model()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEc505o882VL",
        "outputId": "80b65eec-ff63-4959-8828-dff804862c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4690\n",
            "Epoch 2/10, Loss: 0.0529\n",
            "Epoch 3/10, Loss: 0.0181\n",
            "Epoch 4/10, Loss: 0.0093\n",
            "Epoch 5/10, Loss: 0.0052\n",
            "Epoch 6/10, Loss: 0.0087\n",
            "Epoch 7/10, Loss: 0.0026\n",
            "Epoch 8/10, Loss: 0.0019\n",
            "Epoch 9/10, Loss: 0.0048\n",
            "Epoch 10/10, Loss: 0.0011\n",
            "Total evaluation time: 309.27 milliseconds\n",
            "Memory usage after training: 561.82 MB\n",
            "------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-69483d2250f2>:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example user details\n",
        "user_id = 474\n",
        "age = 20\n",
        "occupation = \"scientist\"\n",
        "gender = \"M\"\n",
        "\n",
        "user_features = preprocess_user(age, occupation, gender, checkpoint)\n",
        "\n",
        "movie_to_index = checkpoint['movie_to_index']\n",
        "top_movies, top_scores = get_top_n_recommendations(model, user_features, movie_to_index, n=20)\n",
        "\n",
        "print(f\"Top 20 movie recommendations for user (ID: {user_id}, Age: {age}, Occupation: {occupation}, Gender: {gender}):\")\n",
        "for movie, score in zip(top_movies, top_scores):\n",
        "    print(f\"Movie ID: {movie}, Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ohusFtUhHf",
        "outputId": "5414190e-0d65-4c05-c542-b8fa1b279aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference latency: 5.28 milliseconds\n",
            "Top 20 movie recommendations for user (ID: 474, Age: 20, Occupation: scientist, Gender: M):\n",
            "Movie ID: the+godfather+1972, Score: 47.9227\n",
            "Movie ID: the+english+patient+1996, Score: 47.6316\n",
            "Movie ID: edward+scissorhands+1990, Score: 47.0703\n",
            "Movie ID: starlift+1951, Score: 47.0538\n",
            "Movie ID: harry+potter+and+the+philosophers+stone+2001, Score: 46.9577\n",
            "Movie ID: the+men+who+stare+at+goats+2009, Score: 46.8793\n",
            "Movie ID: iron+man+3+2013, Score: 46.8536\n",
            "Movie ID: the+tin+drum+1979, Score: 46.8247\n",
            "Movie ID: beauty+and+the+beast+1991, Score: 46.8207\n",
            "Movie ID: until+the+end+of+the+world+1991, Score: 46.7858\n",
            "Movie ID: true+lies+1994, Score: 46.7816\n",
            "Movie ID: the+giver+2014, Score: 46.7418\n",
            "Movie ID: the+lord+of+the+rings+the+fellowship+of+the+ring+2001, Score: 46.7133\n",
            "Movie ID: kingsman+the+secret+service+2015, Score: 46.6717\n",
            "Movie ID: the+inner+circle+1991, Score: 46.6531\n",
            "Movie ID: seven+samurai+1954, Score: 46.6071\n",
            "Movie ID: frozen+planet+2011, Score: 46.5982\n",
            "Movie ID: unknown+2011, Score: 46.5909\n",
            "Movie ID: coup+de+torchon+1981, Score: 46.5573\n",
            "Movie ID: les+misrables+1934, Score: 46.5492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPARING SVD AND KNN"
      ],
      "metadata": {
        "id": "hajQJO3dZUeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import Dataset, Reader, SVD, SVDpp, KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from surprise import accuracy\n",
        "import pickle\n",
        "\n",
        "ratings = pd.read_csv('extracted_ratings.csv')\n",
        "watches = pd.read_csv('extracted_watches.csv')\n",
        "movie_details = pd.read_csv('movie_details.csv')\n",
        "user_details = pd.read_csv('user_details.csv')\n",
        "\n",
        "ratings['interaction'] = 1\n",
        "watches['interaction'] = 1\n",
        "interactions = pd.concat([\n",
        "    ratings[['user_id', 'movie_id', 'interaction']],\n",
        "    watches[['user_id', 'movie_id', 'interaction']]\n",
        "]).drop_duplicates()\n",
        "\n",
        "le_gender = LabelEncoder()\n",
        "le_occupation = LabelEncoder()\n",
        "user_details['gender_encoded'] = le_gender.fit_transform(user_details['gender'])\n",
        "user_details['occupation_encoded'] = le_occupation.fit_transform(user_details['occupation'])\n",
        "scaler = StandardScaler()\n",
        "user_details[['age_scaled']] = scaler.fit_transform(user_details[['age']])\n",
        "\n",
        "movie_details['genres'] = movie_details['genres'].fillna('')\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "movie_details['genres_vector'] = tfidf.fit_transform(movie_details['genres']).toarray().tolist()\n",
        "\n",
        "interactions = interactions.merge(user_details, on='user_id', how='left')\n",
        "interactions = interactions.merge(movie_details[['movie_id', 'genres_vector']], on='movie_id', how='left')\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "data = Dataset.load_from_df(interactions[['user_id', 'movie_id', 'interaction']], reader)\n",
        "\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "modelname = 'SVDpp'\n",
        "if modelname == 'SVD':\n",
        "  model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
        "elif modelname == 'SVDpp':\n",
        "  model = SVDpp(n_factors=150, n_epochs=10, lr_all=0.01, reg_all=0.03)\n",
        "elif modelname == 'KNNBasic':\n",
        "  model = KNNBasic(n_neighbors=5)\n",
        "model.fit(trainset)\n",
        "\n",
        "def get_user_features(age, gender, occupation, le_gender, le_occupation, scaler):\n",
        "    age_scaled = scaler.transform([[age]])[0][0] if age is not None else 0\n",
        "    gender_encoded = le_gender.transform([gender])[0] if gender in le_gender.classes_ else -1\n",
        "    occupation_encoded = le_occupation.transform([occupation])[0] if occupation in le_occupation.classes_ else -1\n",
        "    return np.array([age_scaled, gender_encoded, occupation_encoded])\n",
        "\n",
        "def get_movie_features(movie_id):\n",
        "    movie = movie_details[movie_details['movie_id'] == movie_id]\n",
        "    if movie.empty:\n",
        "        return np.zeros(len(tfidf.get_feature_names_out()))\n",
        "    return np.array(movie['genres_vector'].iloc[0])\n",
        "\n",
        "def predict_rating(model, user_id, movie_id, user_features, movie_features):\n",
        "    try:\n",
        "        base_est = model.predict(user_id, movie_id).est\n",
        "    except ValueError:\n",
        "        base_est = model.trainset.global_mean\n",
        "\n",
        "    return max(0, min(1, base_est))\n",
        "\n",
        "def get_top_n_recommendations(model, user_id, age, gender, occupation, movie_details, le_gender, le_occupation, scaler, n=20):\n",
        "    user_features = get_user_features(age, gender, occupation, le_gender, le_occupation, scaler)\n",
        "    user_ratings = []\n",
        "    for _, row in movie_details.iterrows():\n",
        "        movie_id = row['movie_id']\n",
        "        movie_features = np.array(row['genres_vector'])\n",
        "        rating = predict_rating(model, user_id, movie_id, user_features, movie_features)\n",
        "        user_ratings.append((movie_id, rating))\n",
        "\n",
        "    user_ratings = [(movie_id, score + np.random.normal(0, 0.001)) for movie_id, score in user_ratings]\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    return user_ratings[:n]\n",
        "\n",
        "filename = 'recommendation_model_' + modelname + '.pkl'\n",
        "with open(filename, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'svd_model': model,\n",
        "        'le_gender': le_gender,\n",
        "        'le_occupation': le_occupation,\n",
        "        'scaler': scaler,\n",
        "        'tfidf': tfidf\n",
        "    }, f)\n",
        "\n",
        "def server_predict(user_id, age, gender, occupation, movie_details_path):\n",
        "    with open('recommendation_model.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    model = data['svd_model']\n",
        "    le_gender = data['le_gender']\n",
        "    le_occupation = data['le_occupation']\n",
        "    scaler = data['scaler']\n",
        "    tfidf = data['tfidf']\n",
        "\n",
        "    movie_details = pd.read_csv(movie_details_path)\n",
        "    movie_details['genres'] = movie_details['genres'].fillna('')\n",
        "    movie_details['genres_vector'] = tfidf.transform(movie_details['genres']).toarray().tolist()\n",
        "\n",
        "    recommendations = get_top_n_recommendations(model, user_id, age, gender, occupation, movie_details, le_gender, le_occupation, scaler)\n",
        "    return recommendations\n",
        "\n",
        "server_recommendations = server_predict('33039', 30, 'M', 'homemaker', 'movie_details.csv')\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "wbmHakhHZPxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "server_recommendations = server_predict('33095', 27, 'M', 'scientist', 'movie_details.csv')\n",
        "for movie_id, score in server_recommendations[:20]:\n",
        "    print(f\"Movie ID: {movie_id}, Predicted Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "5652LLcUZucw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Define how many times you want to run the inference\n",
        "num_iterations = 20\n",
        "total_inference_time = 0\n",
        "\n",
        "# Loop to get server predictions and calculate inference cost\n",
        "for i in range(num_iterations):\n",
        "    start_time = time.time()  # Record the start time\n",
        "    server_recommendations = server_predict('466', 27, 'F', 'K-12 student', 'movie_details.csv')\n",
        "    end_time = time.time()  # Record the end time\n",
        "\n",
        "    inference_time = end_time - start_time  # Calculate the inference time\n",
        "    total_inference_time += inference_time\n",
        "\n",
        "# Average inference time over all iterations\n",
        "average_inference_time = total_inference_time / num_iterations\n",
        "print(f\"\\nAverage Inference Time: {average_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "YYFUHJXlZyBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "vHIermKYZ33w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}